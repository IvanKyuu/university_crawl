{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import WebDriverException, NoSuchElementException, TimeoutException\n",
    "import time\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Dict\n",
    "\n",
    "import json\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "\n",
    "def store_cache(cache_path, cache: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Stores the provided cache dictionary into a JSON file at the specified path.\n",
    "\n",
    "    Args:\n",
    "        cache_path (str): The file path where the cache will be stored.\n",
    "        cache (dict): The cache data to store.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(cache_path, \"w\", encoding=\"utf-8\") as cache_file:\n",
    "        for key, value in cache.items():\n",
    "            if isinstance(key, tuple):\n",
    "                key = str(key)\n",
    "\n",
    "            json_string = json.dumps({key: value}) + \"\\n\"  # Convert to JSON string with newline\n",
    "            cache_file.write(json_string)\n",
    "\n",
    "\n",
    "class QSRankingCrawl:\n",
    "    base_url = \"https://www.topuniversities.com\"\n",
    "    ranking_page_url = base_url + \"/world-university-rankings&tab=indicators&sort_by=rank&order_by=asc\"\n",
    "    front_url = \"https://www.topuniversities.com/world-university-rankings?page=\"\n",
    "    back_url = \"&tab=indicators&sort_by=rank&order_by=asc\"\n",
    "\n",
    "    def __init__(self, total_num: int = 1498, num_per_page: int = 15):\n",
    "        self.total_num = total_num\n",
    "        self.num_per_page = num_per_page\n",
    "        self.num_page = total_num // num_per_page + 1\n",
    "        self.driver = webdriver.Chrome(service=Service())\n",
    "        self.ranking_info = {}\n",
    "\n",
    "    def get_page_content(self, page_num: int=0):\n",
    "        if page_num > self.num_page:\n",
    "            raise ValueError(f\"Expect a page number <= {self.num_page}, but got page_num: {page_num}\")\n",
    "        try:\n",
    "            self.driver.get(f\"{self.front_url}{page_num}{self.back_url}\")\n",
    "        except TimeoutException:\n",
    "            print(\"Page load timed out. Check your internet connection or website accessibility.\")\n",
    "            return None  # or handle as needed\n",
    "        except WebDriverException as e:\n",
    "            print(f\"An error occurred while trying to navigate: {e}\")\n",
    "            return None\n",
    "        # Full View\n",
    "        # self.driver.find_element(By.XPATH,'//*[@id=\"it-will-be-fixed-top\"]/div/div[1]/div/ul/li[2]/a').click()\n",
    "        return self.driver.page_source\n",
    "    \n",
    "    def parse_page(self, page_content):\n",
    "        soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "        rows = soup.find_all(name=\"div\", attrs={\"class\": \"row ind-row firstloaded hide-this-in-mobile-indi\"})\n",
    "        page_data = {}\n",
    "        for row in rows:\n",
    "            row_bs = BeautifulSoup(str(row), \"html.parser\")\n",
    "            uni_link = row_bs.find(name=\"a\")\n",
    "            university_name = uni_link.text.strip()\n",
    "            link = uni_link[\"href\"]\n",
    "            rank = row_bs.find(name=\"div\", attrs={\"class\": \"_univ-rank mw-100\"}).text\n",
    "            # print(university_name)\n",
    "            page_data[university_name] = {\"rank\": rank, \"uni_link\": f\"{self.base_url}{link}\"}\n",
    "            self.ranking_info.update(page_data)\n",
    "        return page_data\n",
    "\n",
    "    def get_all_ranking(self):\n",
    "        for i in range(self.num_page):\n",
    "            self.parse_page(self.get_page_content(i))\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        df = pd.DataFrame.from_dict(self.ranking_info, orient=\"index\").reset_index()\n",
    "        df.columns = ['university_name', 'rank', 'qs_uni_link']\n",
    "        return df\n",
    "    \n",
    "    def to_csv(self):\n",
    "        df = self.to_dataframe()\n",
    "        file_path = \"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/ranking_fetcher/ranking_data/qs_ranking_2024.csv\"\n",
    "        df.to_csv(file_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cra = QSRankingCrawl()\n",
    "    # pprint(cra.get_page_content(0))\n",
    "    cra.get_all_ranking()\n",
    "    try:\n",
    "        file_path = \"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/ranking_fetcher/ranking_data/qs_ranking_2024.jsonl\"\n",
    "        store_cache(f\"{file_path}\", cra.ranking_info)\n",
    "    except IOError as exc:\n",
    "        raise IOError(f\"An error occurred while writing to the file: {file_path}\") from exc\n",
    "    finally:\n",
    "        cra.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/ranking_fetcher/ranking_data/qs_ranking_2024.jsonl\"\n",
    "store_cache(f\"{file_path}\", cra.ranking_info)\n",
    "# pprint(cra.ranking_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(cra.num_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cra.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 设置浏览器内核路径\n",
    "# servic = Service()\n",
    "# driver = webdriver.Chrome(service=servic)\n",
    "# # 获取网页\n",
    "# base_url = \"https://www.topuniversities.com\"\n",
    "# driver.get(\"https://www.topuniversities.com/world-university-rankings?page=1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.find_element(By.XPATH,'//*[@id=\"it-will-be-fixed-top\"]/div/div[1]/div/ul/li[2]/a').click()\n",
    "# time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni_info = {}\n",
    "# soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "# rows = soup.find_all(name=\"div\", attrs={\"class\": \"row ind-row firstloaded hide-this-in-mobile-indi\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = []\n",
    "# for row in rows:\n",
    "#     row_bs = BeautifulSoup(str(row))\n",
    "#     uni_link = row_bs.find(name=\"a\")\n",
    "#     university_name = uni_link.text.strip()\n",
    "#     link = uni_link[\"href\"]\n",
    "#     rank = row_bs.find(name=\"div\", attrs={\"class\": \"_univ-rank mw-100\"}).text\n",
    "#     uni_info.update({university_name: {\"rank\": rank, \"uni_link\": f\"{base_url}{link}\"}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(uni_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_one_page(uni_info):\n",
    "#     for row in rows:\n",
    "#         row_bs = BeautifulSoup(str(row))\n",
    "#         uni_link = row_bs.find(name=\"a\")\n",
    "#         university_name = uni_link.text.strip()\n",
    "#         link = uni_link[\"href\"]\n",
    "#         rank = row_bs.find(name=\"div\", attrs={\"class\": \"_univ-rank mw-100\"}).text\n",
    "#         uni_info.update({university_name: {\"rank\": rank, \"uni_link\": f\"{base_url}{link}\"}})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
