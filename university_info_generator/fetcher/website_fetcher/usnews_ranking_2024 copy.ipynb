{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import WebDriverException, NoSuchElementException, TimeoutException\n",
    "import time\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Dict, List\n",
    "\n",
    "import json\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import threading\n",
    "\n",
    "\n",
    "def store_cache(cache_path, cache: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Stores the provided cache dictionary into a JSON file at the specified path.\n",
    "\n",
    "    Args:\n",
    "        cache_path (str): The file path where the cache will be stored.\n",
    "        cache (dict): The cache data to store.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(cache_path, \"a\", encoding=\"utf-8\") as cache_file:\n",
    "        for key, value in cache.items():\n",
    "            if isinstance(key, tuple):\n",
    "                key = str(key)\n",
    "\n",
    "            json_string = json.dumps({key: value}) + \"\\n\"  # Convert to JSON string with newline\n",
    "            cache_file.write(json_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "class USNewsRankingCrawl:\n",
    "    base_url = \"https://www.usnews.com\"\n",
    "    ranking_page_url = base_url + \"/education/best-global-universities/canada\"\n",
    "    front_url = \"?name=\"\n",
    "    # back_url = \"&country=api&subject=search\"\n",
    "\n",
    "    def __init__(self, total_num: int = 2165):\n",
    "        self.total_num = total_num\n",
    "        self.driver = webdriver.Chrome(service=Service())\n",
    "        self.ranking_info = {}\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    # TODO: fix this, can't load all pages, dynamically, at once\n",
    "    def keep_scrolling_to_the_bottom(self):\n",
    "        while True:\n",
    "            previous_scrollY = self.driver.execute_script( 'return window.scrollY' )\n",
    "            self.driver.execute_script( 'window.scrollBy( 0, 230 )' )\n",
    "            time.sleep( 10 )\n",
    "            if previous_scrollY == self.driver.execute_script( 'return window.scrollY' ):\n",
    "                button = self.driver.find_element(By.CSS_SELECTOR, \"#rankings > div.pager__Container-sc-1i8e93j-0.hqeWub > button\")\n",
    "                button.click()\n",
    "                time.sleep( 2 )\n",
    "                if previous_scrollY == self.driver.execute_script( 'return window.scrollY' ):\n",
    "                    break\n",
    "\n",
    "\n",
    "    def get_page_content_by_name(self, name:str):\n",
    "        if not name:\n",
    "            raise ValueError(\"Expect the name of a university but got none.\")\n",
    "        try:\n",
    "            self.driver.get(f\"{self.ranking_page_url}{self.front_url}{name}\")\n",
    "        except TimeoutException:\n",
    "            print(\"Page load timed out. Check your internet connection or website accessibility.\")\n",
    "            return None  # or handle as needed\n",
    "        except WebDriverException as e:\n",
    "            print(f\"An error occurred while trying to navigate: {e}\")\n",
    "            return None\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_page(self):\n",
    "        self.driver.get(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/ranking_fetcher/ranking_data/usnews_canada_2023.html\")\n",
    "\n",
    "    def parse_page(self, page_content):\n",
    "        page_data = {}\n",
    "        soup = BeautifulSoup(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/ranking_fetcher/ranking_data/usnews_canada_2023.html\", \"html.parser\")\n",
    "        try:\n",
    "            rows = soup.find_all(name=\"li\", attrs={\"class\": \"item-list__ListItemStyled-sc-18yjqdy-1 boZDDO\"})\n",
    "        except AttributeError:\n",
    "            return\n",
    "        for row in rows:\n",
    "            with self.lock:\n",
    "                row_bs = BeautifulSoup(str(row), \"html.parser\")\n",
    "                try:\n",
    "                    rank = row_bs.find(name=\"div\", attrs={\"class\": \"RankList__Rank-sc-2xewen-2 ieuiBj ranked has-badge\"}).text.strip()\n",
    "                    uni_link = row_bs.find(name=\"a\", attrs={\"class\": \"Anchor-byh49a-0 DetailCardGlobalUniversities__StyledAnchor-sc-1v60hm5-5 eMEqFO bFdMFJ\"})\n",
    "                    university_name = uni_link.text.strip()\n",
    "                    link = uni_link[\"href\"]\n",
    "                    if \"#\" in rank:\n",
    "                        rank = rank[rank.index(\"#\")+1:]\n",
    "                except AttributeError:\n",
    "                    print(\"Not enough info skipped\\n\" + str(row))\n",
    "                    continue\n",
    "                page_data[university_name] = {\"rank\": rank, \"uni_link\": f\"{link}\"}\n",
    "                # print(page_data[university_name])\n",
    "\n",
    "                self.ranking_info.update(page_data)\n",
    "        return page_data\n",
    "\n",
    "    def get_all_ranking_by_list(self, lst:List[str]):\n",
    "            self.parse_page(self.get_page())\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        df = pd.DataFrame.from_dict(self.ranking_info, orient=\"index\").reset_index()\n",
    "        df.columns = [\"university_name\", \"rank\", \"arwu_uni_link\"]\n",
    "        return df\n",
    "\n",
    "    def to_csv(self):\n",
    "        df = self.to_dataframe()\n",
    "        file_path = \"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/ranking_fetcher/ranking_data/us_news_ranking_2023.csv\"\n",
    "        df.to_csv(file_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"all_universities.csv\")\n",
    "# lst = df[\"university_name\"].to_list()\n",
    "# print(lst[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/anaconda3/envs/gpu/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cra = USNewsRankingCrawl()\n",
    "    # pprint(cra.get_page_content(0))\n",
    "    page = cra.parse_page(None)\n",
    "    try:\n",
    "        file_path = \"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/ranking_fetcher/ranking_data/us_news_ranking_2023.jsonl\"\n",
    "        store_cache(f\"{file_path}\", cra.ranking_info)\n",
    "    except IOError as exc:\n",
    "        raise IOError(f\"An error occurred while writing to the file: {file_path}\") from exc\n",
    "    finally:\n",
    "        cra.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_cache(f\"{file_path}\", cra.ranking_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(cra.ranking_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/ranking_fetcher/ranking_data/usnews_canada_2023.html\", \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup.find_all(name=\"li\", attrs={\"class\": \"item-list__ListItemStyled-sc-18yjqdy-1 boZDDO\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(rows))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
