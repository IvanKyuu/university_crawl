{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import WebDriverException, NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "from typing import Dict, List\n",
    "\n",
    "import json\n",
    "import threading\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "\n",
    "def store_cache(cache_path, cache: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Stores the provided cache dictionary into a JSON file at the specified path.\n",
    "\n",
    "    Args:\n",
    "        cache_path (str): The file path where the cache will be stored.\n",
    "        cache (dict): The cache data to store.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(cache_path, \"w\", encoding=\"utf-8\") as cache_file:\n",
    "        for key, value in cache.items():\n",
    "            if isinstance(key, tuple):\n",
    "                key = str(key)\n",
    "\n",
    "            json_string = json.dumps({key: value}) + \"\\n\"  # Convert to JSON string with newline\n",
    "            cache_file.write(json_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "class ARWURankingCrawl:\n",
    "\n",
    "    base_url = \"https://www.shanghairanking.com\"\n",
    "    ranking_page_url = base_url + \"/rankings/arwu/2023\"\n",
    "\n",
    "    def __init__(self, total_num: int = 1000, num_per_page: int = 30):\n",
    "        self.total_num = total_num\n",
    "        self.num_per_page = num_per_page\n",
    "        self.num_page = total_num // num_per_page + 1\n",
    "        # System.setProperty(\"webdriver.chrome.driver\", \"C:\\\\driver\\\\chromedriver.exe\")\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.ranking_info = {}\n",
    "        self.lock = threading.Lock()\n",
    "        self.df_path = \"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/arwu_ranking_2023.csv\"\n",
    "\n",
    "    def get_first_page(self, url=ranking_page_url):\n",
    "        try:\n",
    "            self.driver.get(f\"{url}\")\n",
    "        except TimeoutException:\n",
    "            print(\"Page load timed out. Check your internet connection or website accessibility.\")\n",
    "            return None  # or handle as needed\n",
    "        except WebDriverException as e:\n",
    "            print(f\"An error occurred while trying to navigate: {e}\")\n",
    "            return None\n",
    "        # Full View\n",
    "        # self.driver.find_element(By.XPATH,'//*[@id=\"it-will-be-fixed-top\"]/div/div[1]/div/ul/li[2]/a').click()\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_page_content(self):\n",
    "        try:\n",
    "            self.driver.find_element(By.CSS_SELECTOR, \"#content-box > ul > li.ant-pagination-next\").click()\n",
    "        except TimeoutException:\n",
    "            print(\"Page load timed out. Check your internet connection or website accessibility.\")\n",
    "            return None  # or handle as needed\n",
    "        except WebDriverException as e:\n",
    "            print(f\"An error occurred while trying to navigate: {e}\")\n",
    "            return None\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def parse_page(self, page_content):\n",
    "        page_data = {}\n",
    "\n",
    "        soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "        rows = soup.find_all(name=\"tr\")\n",
    "        for row in rows[-self.num_per_page :]:\n",
    "            # print(row)\n",
    "            row_bs = BeautifulSoup(str(row), \"html.parser\")\n",
    "            try:\n",
    "                rank = row_bs.find(name=\"div\", attrs={\"class\": \"ranking\"}).text.strip()\n",
    "                link = row_bs.find(name=\"a\")[\"href\"]\n",
    "                university_name = row_bs.find(name=\"span\", attrs={\"class\": \"univ-name\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                print(\"Not enough info skipped\\n\" + str(row))\n",
    "                continue\n",
    "            page_data[university_name] = {\"rank\": rank, \"uni_link\": f\"{self.base_url}{link}\"}\n",
    "            # print(page_data[university_name])\n",
    "            self.ranking_info.update(page_data)\n",
    "        return page_data\n",
    "\n",
    "    def get_all_ranking(self):\n",
    "        with self.lock:\n",
    "            self.parse_page(self.get_first_page())\n",
    "        for _ in range(1, self.num_page + 1):\n",
    "            with self.lock:\n",
    "                self.parse_page(self.get_page_content())\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        df = pd.DataFrame.from_dict(self.ranking_info, orient=\"index\").reset_index()\n",
    "        df.columns = [\"university_name\", \"rank\", \"arwu_uni_link\"]\n",
    "        return df\n",
    "\n",
    "    def to_csv(self):\n",
    "        df = self.to_dataframe()\n",
    "        file_path = \"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/arwu_ranking_2023.csv\"\n",
    "        df.to_csv(file_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    def filter_on_df(self, target_unis: pd.DataFrame):\n",
    "        if os.path.exists(self.df_path):\n",
    "            # File exists, load it into a DataFrame\n",
    "            arwu_data_frame = pd.read_csv(self.df_path)\n",
    "        else:\n",
    "            arwu_data_frame = self.to_dataframe()\n",
    "        # target_uni_canada = pd.read_csv(path_to_csv)\n",
    "        arwu_data_frame[\"university_name_lower\"] = arwu_data_frame[\"university_name\"].str.lower()\n",
    "        target_unis[\"university_name_lower\"] = target_unis[\"university_name\"].str.lower()\n",
    "        column_name = [\"canada_id\", \"arwu_name\", \"canada_name\", \"arwu_rank\", \"arwu_link\"]\n",
    "        matched_df = pd.DataFrame(columns=column_name)\n",
    "        for _, arwu_row in arwu_data_frame.iterrows():\n",
    "            arwu_name = arwu_row[\"university_name_lower\"]\n",
    "\n",
    "            # Check each university in the Canadian DataFrame for a substring match\n",
    "            for _, canada_row in target_unis.iterrows():\n",
    "                canada_name = canada_row[\"university_name_lower\"]\n",
    "                # Check if one name is a substring of the other\n",
    "                if arwu_name in canada_name or canada_name in arwu_name:\n",
    "                    # Add the match to the DataFrame\n",
    "                    matched_df = pd.concat(\n",
    "                        [\n",
    "                            matched_df,\n",
    "                            pd.DataFrame(\n",
    "                                [\n",
    "                                    [\n",
    "                                        canada_row[\"id_\"],\n",
    "                                        arwu_row[\"university_name\"],\n",
    "                                        canada_row[\"university_name\"],\n",
    "                                        arwu_row[\"rank\"],\n",
    "                                        arwu_row[\"arwu_uni_link\"],\n",
    "                                    ]\n",
    "                                ],\n",
    "                                columns=column_name,\n",
    "                            ),\n",
    "                        ],\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "        matched_df.drop_duplicates(subset=[\"arwu_name\"], keep=\"first\", inplace=True)\n",
    "        return matched_df\n",
    "\n",
    "    def get_programs(self, url) -> List[str]:\n",
    "        def filter_target_table(tables):\n",
    "            target_tables = list(\n",
    "                filter(\n",
    "                    lambda table: \"undergraduate programs\" in table.select(\"table > thead > tr > th\")[0].string.lower(),\n",
    "                    tables,\n",
    "                )\n",
    "            )\n",
    "            if target_tables and len(target_tables) > 0:\n",
    "                return target_tables[0]\n",
    "            return None\n",
    "\n",
    "        result_lst = []\n",
    "        try:\n",
    "            # page = requests.get(f\"{url}\")\n",
    "            # soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            self.driver.get(f\"{url}\")\n",
    "            soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "            tables = soup.find_all(name=\"table\")\n",
    "            target_table = filter_target_table(tables)\n",
    "            if not target_table:\n",
    "                return []\n",
    "            table_soup = BeautifulSoup(str(target_table), \"html.parser\")\n",
    "            rows = table_soup.find(\n",
    "                name=\"tbody\",\n",
    "            ).find_all(\n",
    "                name=\"tr\",\n",
    "            )\n",
    "            for row in rows:\n",
    "                result_lst.append(row.string.strip())\n",
    "        except TimeoutException:\n",
    "            print(\"Page load timed out. Check your internet connection or website accessibility.\")\n",
    "        except WebDriverException as e:\n",
    "            print(f\"An error occurred while trying to navigate: {e}\")\n",
    "        if result_lst:\n",
    "            result_lst = list(set(result_lst))\n",
    "            result_lst.sort()\n",
    "        return result_lst\n",
    "\n",
    "    def get_all_programs(self, dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        # matched_universities[[\"arwu_name\", \"arwu_link\"]]\n",
    "        column_name = [\"canada_id\", \"arwu_name\", \"canada_name\", \"programs\"]\n",
    "        result_df = pd.DataFrame(columns=column_name)\n",
    "        for _, row in dataframe.iterrows():\n",
    "            result_lst = self.get_programs(row[\"arwu_link\"])\n",
    "            if result_lst:\n",
    "                row_df = pd.DataFrame(\n",
    "                            [\n",
    "                                [\n",
    "                                    row[\"canada_id\"],\n",
    "                                    row[\"arwu_name\"],\n",
    "                                    row[\"canada_name\"],\n",
    "                                    f\"{result_lst}\",\n",
    "                                ]\n",
    "                            ],\n",
    "                            columns=column_name,\n",
    "                        )\n",
    "                result_df = pd.concat(\n",
    "                    [\n",
    "                        result_df,\n",
    "                        row_df,\n",
    "                    ],\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     cra = ARWURankingCrawl()\n",
    "#     # pprint(cra.get_page_content(0))\n",
    "#     page = cra.get_all_ranking()\n",
    "#     pprint(page)\n",
    "#     try:\n",
    "#         file_path = \"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/arwu_ranking_2023.jsonl\"\n",
    "#         store_cache(f\"{file_path}\", cra.ranking_info)\n",
    "#     except IOError as exc:\n",
    "#         raise IOError(f\"An error occurred while writing to the file: {file_path}\") from exc\n",
    "#     finally:\n",
    "#         cra.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cra = ARWURankingCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_data = pd.read_csv(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/canada_dataset/all_universities_canada.csv\")\n",
    "# canada_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_data = pd.read_csv(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/all_universities_canada.csv\")\n",
    "# canada_matched_universities = cra.filter_on_df(canada_data)\n",
    "# canada_matched_universities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_matched_universities.to_csv(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/canada_dataset/canada_uni_matched.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_matched_universities.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_result_df = cra.get_all_programs(canada_matched_universities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_result_df.to_csv(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/canada_dataset/canada_programs.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canada_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # canada_result_df['programs'] = canada_result_df['programs'].apply(eval)\n",
    "# # canada_result_df.head()\n",
    "# # Explode the 'programs' column so each program gets its own row\n",
    "# df_exploded = canada_result_df.explode('programs')\n",
    "# df_exploded.to_csv(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/canada_dataset/canada_programs_eval.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_data = pd.read_csv(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/all_universities_usa.csv\")\n",
    "usa_matched_universities = cra.filter_on_df(usa_data)\n",
    "usa_matched_universities.head()\n",
    "usa_result_df = cra.get_all_programs(usa_matched_universities)\n",
    "usa_result_df.to_csv(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/usa_programs.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_matched_universities.to_csv(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/usa_uni_matched.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_result_df['programs'] = usa_result_df['programs'].apply(eval)\n",
    "\n",
    "# Explode the 'programs' column so each program gets its own row\n",
    "df_exploded = usa_result_df.explode('programs')\n",
    "df_exploded.to_csv(\"/home/ivan/Uforse/university_crawl/university_info_generator/fetcher/website_fetcher/ranking_data/usa_programs_eval.csv\", encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
